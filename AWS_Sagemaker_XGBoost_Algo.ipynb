{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2a7c68d",
   "metadata": {},
   "source": [
    "## Importing Important Libraries\n",
    "### Steps To Be Followed\n",
    "\n",
    "    Importing necessary Libraries\n",
    "    Creating S3 bucket\n",
    "    Mapping train And Test Data in S3\n",
    "    Mapping The path of the models in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddf3f2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/base_serializers.py:28: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.22.4)\n",
      "  import scipy.sparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3a93103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eu-west-2\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"llamamodelsfortranslation\"\n",
    "my_region =boto3.session.Session().region_name\n",
    "print(my_region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7f8fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'X3CVCDRGJR3ZR483',\n",
       "  'HostId': '8LYhIonYNwvKSet9NxcZglCgNI3J/X071hqj9APsmciBKvG8+X+qmP7MhqEI1RJ4B9bAilCDd1o=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '8LYhIonYNwvKSet9NxcZglCgNI3J/X071hqj9APsmciBKvG8+X+qmP7MhqEI1RJ4B9bAilCDd1o=',\n",
       "   'x-amz-request-id': 'X3CVCDRGJR3ZR483',\n",
       "   'date': 'Thu, 05 Sep 2024 20:02:48 GMT',\n",
       "   'location': 'http://llamamodelsfortranslation.s3.amazonaws.com/',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'Location': 'http://llamamodelsfortranslation.s3.amazonaws.com/'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3', region_name='eu-west-2')\n",
    "\n",
    "s3.create_bucket(\n",
    "    Bucket='llamamodelsfortranslation',\n",
    "    CreateBucketConfiguration={\n",
    "        'LocationConstraint': 'eu-west-2'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b25a001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://llamamodelsfortranslation/sample/train\n"
     ]
    }
   ],
   "source": [
    "prefix = \"sample\"\n",
    "output_path = \"s3://{}/{}/train\".format(bucket_name ,prefix)\n",
    "print(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8391d2",
   "metadata": {},
   "source": [
    "## Downloading The Dataset And Storing in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b00497df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: downloaded bank_clean.csv.\n",
      "Success: Data loaded into dataframe.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "try:\n",
    "    urllib.request.urlretrieve (\"https://d1.awsstatic.com/tmt/build-train-deploy-machine-learning-model-sagemaker/bank_clean.27f01fbbdf43271788427f3682996ae29ceca05d.csv\", \"bank_clean.csv\")\n",
    "    print('Success: downloaded bank_clean.csv.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)\n",
    "\n",
    "try:\n",
    "    model_data = pd.read_csv('./bank_clean.csv',index_col=0)\n",
    "    print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58450619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28831, 61) (12357, 61)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Train Test split\n",
    "\n",
    "import numpy as np\n",
    "train_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data))])\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0008c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and save the training data as CSV\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "\n",
    "# Upload the CSV file to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "\n",
    "# Define S3 input for SageMaker (replacing the deprecated s3_input)\n",
    "s3_input_train = TrainingInput(s3_data='s3://{}/{}/train/train.csv'.format(bucket_name, prefix), content_type='csv')\n",
    "\n",
    "# Now s3_input_train can be used for training in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8e4df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and save the testing data as CSV\n",
    "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)\n",
    "\n",
    "# Upload the CSV file to S3\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'test/test.csv')).upload_file('test.csv')\n",
    "\n",
    "# Define S3 input for SageMaker (replacing the deprecated s3_input)\n",
    "s3_input_test = TrainingInput(s3_data='s3://{}/{}/test/test.csv'.format(bucket_name, prefix), content_type='csv')\n",
    "\n",
    "# Now s3_input_test can be used for testing in SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87550185",
   "metadata": {},
   "source": [
    "## Building Models Xgboot- Inbuilt Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22275908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost container URI: 764974769150.dkr.ecr.eu-west-2.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Use the newer `sagemaker.image_uris.retrieve` method\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    framework='xgboost', \n",
    "    region=boto3.Session().region_name, \n",
    "    version='1.0-1'\n",
    ")\n",
    "\n",
    "print(f'XGBoost container URI: {container}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f8720b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":50\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efa19f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimator created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Construct a SageMaker estimator using the updated SageMaker SDK\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=container,  # Changed from image_name to image_uri\n",
    "    hyperparameters=hyperparameters,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,  \n",
    "    instance_type='ml.m5.2xlarge',  \n",
    "    volume_size=5,  \n",
    "    output_path=output_path,\n",
    "    use_spot_instances=True,  \n",
    "    max_run=300, \n",
    "    max_wait=600  \n",
    ")\n",
    "\n",
    "print(\"Estimator created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02d99163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-09-05-20-49-01-674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-05 20:49:07 Starting - Starting the training job...\n",
      "2024-09-05 20:49:23 Starting - Preparing the instances for training...\n",
      "2024-09-05 20:49:58 Downloading - Downloading the training image...\n",
      "2024-09-05 20:50:34 Training - Training image download completed. Training in progress...\u001b[34m[2024-09-05 20:50:50.996 ip-10-0-185-35.eu-west-2.compute.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[20:50:51] 28831x59 matrix with 1701029 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[20:50:51] 12357x59 matrix with 729063 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2024-09-05 20:50:51.168 ip-10-0-185-35.eu-west-2.compute.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2024-09-05 20:50:51.169 ip-10-0-185-35.eu-west-2.compute.internal:7 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2024-09-05 20:50:51.169 ip-10-0-185-35.eu-west-2.compute.internal:7 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-09-05 20:50:51.170 ip-10-0-185-35.eu-west-2.compute.internal:7 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2024-09-05 20:50:51.170 ip-10-0-185-35.eu-west-2.compute.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 28831 rows\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 12357 rows\u001b[0m\n",
      "\u001b[34m[20:50:51] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { num_round } might not be used.\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.10079#011validation-error:0.10528\u001b[0m\n",
      "\u001b[34m[2024-09-05 20:50:51.213 ip-10-0-185-35.eu-west-2.compute.internal:7 INFO hook.py:423] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2024-09-05 20:50:51.214 ip-10-0-185-35.eu-west-2.compute.internal:7 INFO hook.py:486] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.09968#011validation-error:0.10456\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.10017#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.09989#011validation-error:0.10310\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.09996#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.09906#011validation-error:0.10261\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.09930#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.09951#011validation-error:0.10261\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.09920#011validation-error:0.10286\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.09871#011validation-error:0.10294\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.09868#011validation-error:0.10294\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.09868#011validation-error:0.10326\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.09854#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.09892#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.09850#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.09844#011validation-error:0.10326\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.09857#011validation-error:0.10318\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.09799#011validation-error:0.10318\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.09816#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.09857#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.09830#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.09826#011validation-error:0.10318\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.09847#011validation-error:0.10399\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.09833#011validation-error:0.10407\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.09812#011validation-error:0.10415\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.09812#011validation-error:0.10399\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.09774#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.09781#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.09781#011validation-error:0.10391\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.09778#011validation-error:0.10367\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.09781#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.09771#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.09743#011validation-error:0.10391\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.09753#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.09767#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.09757#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.09757#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.09736#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.09750#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.09733#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.09705#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.09701#011validation-error:0.10383\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.09712#011validation-error:0.10407\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.09698#011validation-error:0.10375\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.09733#011validation-error:0.10342\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.09736#011validation-error:0.10367\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.09746#011validation-error:0.10350\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.09736#011validation-error:0.10358\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.09712#011validation-error:0.10334\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.09712#011validation-error:0.10318\u001b[0m\n",
      "\n",
      "2024-09-05 20:51:07 Uploading - Uploading generated training model\n",
      "2024-09-05 20:51:07 Completed - Training job completed\n",
      "Training seconds: 84\n",
      "Billable seconds: 31\n",
      "Managed Spot Training savings: 63.1%\n",
      "Training started!\n"
     ]
    }
   ],
   "source": [
    "# Assuming s3_input_train and s3_input_test are already defined using TrainingInput\n",
    "estimator.fit({'train': s3_input_train, 'validation': s3_input_test})\n",
    "\n",
    "print(\"Training started!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378d3b9",
   "metadata": {},
   "source": [
    "## Deploy Machine Learning Model As Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0997846b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2024-09-05-20-54-26-404\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2024-09-05-20-54-26-404\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2024-09-05-20-54-26-404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!Model deployed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Deploy the model to a SageMaker endpoint\n",
    "xgb_predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.m4.xlarge'\n",
    ")\n",
    "\n",
    "print(\"Model deployed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab8c271",
   "metadata": {},
   "source": [
    "## Prediction of the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47c4f0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12357,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "# Prepare test data by removing unwanted columns\n",
    "test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values\n",
    "\n",
    "# Configure the predictor\n",
    "xgb_predictor.content_type = 'text/csv'  # Set content type for inference\n",
    "xgb_predictor.serializer = CSVSerializer()  # Use CSVSerializer from the new SageMaker SDK\n",
    "\n",
    "# Make predictions\n",
    "predictions = xgb_predictor.predict(test_data_array).decode('utf-8')\n",
    "\n",
    "# Convert predictions from string to a numpy array\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "# Print the shape of the predictions array\n",
    "print(predictions_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdc95340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05214286, 0.05660191, 0.05096195, ..., 0.03436061, 0.02942475,\n",
       "       0.03715819])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a9a720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Classification Rate: 89.7%\n",
      "\n",
      "Predicted      No Purchase    Purchase\n",
      "Observed\n",
      "No Purchase    91% (10785)    34% (151)\n",
      "Purchase        9% (1124)     66% (297)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = pd.crosstab(index=test_data['y_yes'], \n",
    "                 columns=np.round(predictions_array), \n",
    "                 rownames=['Observed'], \n",
    "                 colnames=['Predicted'])\n",
    "\n",
    "# Extract true positives, false negatives, true negatives, and false positives\n",
    "tn = cm.iloc[0, 0]\n",
    "fn = cm.iloc[1, 0]\n",
    "tp = cm.iloc[1, 1]\n",
    "fp = cm.iloc[0, 1]\n",
    "\n",
    "# Calculate the overall classification rate\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn) * 100\n",
    "\n",
    "# Print the results\n",
    "print(\"\\n{0:<25}{1:<4.1f}%\\n\".format(\"Overall Classification Rate: \", accuracy))\n",
    "print(\"{0:<15}{1:<15}{2:>8}\".format(\"Predicted\", \"No Purchase\", \"Purchase\"))\n",
    "print(\"Observed\")\n",
    "print(\"{0:<15}{1:<2.0f}% ({2:<}){3:>6.0f}% ({4:<})\".format(\n",
    "    \"No Purchase\", tn / (tn + fn) * 100, tn, fp / (tp + fp) * 100, fp))\n",
    "print(\"{0:<16}{1:<1.0f}% ({2:<}){3:>7.0f}% ({4:<})\\n\".format(\n",
    "    \"Purchase\", fn / (tn + fn) * 100, fn, tp / (tp + fp) * 100, tp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513ff71",
   "metadata": {},
   "source": [
    "## Deleting The Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f4e9b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-xgboost-2024-09-05-20-54-26-404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint and bucket contents deleted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Delete the SageMaker endpoint\n",
    "sagemaker.Session().delete_endpoint(xgb_predictor.endpoint_name)\n",
    "# Delete all objects in the S3 bucket\n",
    "bucket_to_delete = boto3.resource('s3').Bucket(bucket_name)\n",
    "bucket_to_delete.objects.delete()\n",
    "print(\"Endpoint and bucket contents deleted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a37e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
